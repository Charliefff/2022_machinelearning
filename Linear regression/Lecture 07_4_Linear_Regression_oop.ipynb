{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.001, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.matmul(X, self.weights)\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        return np.average((self.predict(X) - Y) ** 2)\n",
    "\n",
    "    def gradient(self, X, Y):\n",
    "        return 2 * np.matmul(X.T, (self.predict(X) - Y)) / X.shape[0]\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        self.weights = np.zeros((X.shape[1], 1))\n",
    "        for i in range(self.n_iters):\n",
    "            print(\"Iteration %4d => Loss: %.20f\" % (i, self.loss(X, Y)))\n",
    "            self.weights -= self.gradient(X, Y) * self.lr            \n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = w1x1 + w2x2 + w3x3 + b = b * 1 + w1x1 + w2x2 + w3x3 where we consider b as w0 and 1 as x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 1333.56666666666660603369\n",
      "Iteration    1 => Loss: 152.37148173674074769224\n",
      "Iteration    2 => Loss: 65.17252143398701491606\n",
      "Iteration    3 => Loss: 57.29348107043314541897\n",
      "Iteration    4 => Loss: 55.24408375010615657175\n",
      "Iteration    5 => Loss: 53.69548034496938981874\n",
      "Iteration    6 => Loss: 52.25567333361470900854\n",
      "Iteration    7 => Loss: 50.89073275996360479212\n",
      "Iteration    8 => Loss: 49.59315053477826751305\n",
      "Iteration    9 => Loss: 48.35777747932070269599\n",
      "Iteration   10 => Loss: 47.18003972981339444459\n",
      "Iteration   11 => Loss: 46.05577241746296834890\n",
      "Iteration   12 => Loss: 44.98117392026740901656\n",
      "Iteration   13 => Loss: 43.95277197431995119814\n",
      "Iteration   14 => Loss: 42.96739348065430874613\n",
      "Iteration   15 => Loss: 42.02213715248832670568\n",
      "Iteration   16 => Loss: 41.11434870159364152187\n",
      "Iteration   17 => Loss: 40.24159832491246646669\n",
      "Iteration   18 => Loss: 39.40166027726749575777\n",
      "Iteration   19 => Loss: 38.59249433612598778609\n",
      "Iteration   20 => Loss: 37.81222898245525954053\n",
      "Iteration   21 => Loss: 37.05914613808703705899\n",
      "Iteration   22 => Loss: 36.33166731486301870291\n",
      "Iteration   23 => Loss: 35.62834104430512383033\n",
      "Iteration   24 => Loss: 34.94783146877178126033\n",
      "Iteration   25 => Loss: 34.28890798614055768212\n",
      "Iteration   26 => Loss: 33.65043585010660365242\n",
      "Iteration   27 => Loss: 33.03136763729814617818\n",
      "Iteration   28 => Loss: 32.43073550067536814367\n",
      "Iteration   29 => Loss: 31.84764413617337197593\n",
      "Iteration   30 => Loss: 31.28126439634724320626\n",
      "Iteration   31 => Loss: 30.73082749094157861691\n",
      "Iteration   32 => Loss: 30.19561971989697468643\n",
      "Iteration   33 => Loss: 29.67497768937651159149\n",
      "Iteration   34 => Loss: 29.16828396599243689025\n",
      "Iteration   35 => Loss: 28.67496312858387241818\n",
      "Iteration   36 => Loss: 28.19447818067768807282\n",
      "Iteration   37 => Loss: 27.72632729019449016050\n",
      "Iteration   38 => Loss: 27.27004082607204793476\n",
      "Iteration   39 => Loss: 26.82517866429934016992\n",
      "Iteration   40 => Loss: 26.39132773841262391556\n",
      "Iteration   41 => Loss: 25.96809981182500237651\n",
      "Iteration   42 => Loss: 25.55512945146534065088\n",
      "Iteration   43 => Loss: 25.15207218411042688899\n",
      "Iteration   44 => Loss: 24.75860281852498800959\n",
      "Iteration   45 => Loss: 24.37441391809391078027\n",
      "Iteration   46 => Loss: 23.99921441005418998316\n",
      "Iteration   47 => Loss: 23.63272831872565760136\n",
      "Iteration   48 => Loss: 23.27469361130964031759\n",
      "Iteration   49 => Loss: 22.92486114588736967335\n",
      "Iteration   50 => Loss: 22.58299371221250950725\n",
      "Iteration   51 => Loss: 22.24886515676563902844\n",
      "Iteration   52 => Loss: 21.92225958433098043088\n",
      "Iteration   53 => Loss: 21.60297062907392273701\n",
      "Iteration   54 => Loss: 21.29080078874931913901\n",
      "Iteration   55 => Loss: 20.98556081626193048351\n",
      "Iteration   56 => Loss: 20.68706916333597689572\n",
      "Iteration   57 => Loss: 20.39515147153707985694\n",
      "Iteration   58 => Loss: 20.10964010633085052859\n",
      "Iteration   59 => Loss: 19.83037373026215988148\n",
      "Iteration   60 => Loss: 19.55719691170171969929\n",
      "Iteration   61 => Loss: 19.28995976593591166193\n",
      "Iteration   62 => Loss: 19.02851762567397742032\n",
      "Iteration   63 => Loss: 18.77273073831701211134\n",
      "Iteration   64 => Loss: 18.52246398757959511272\n",
      "Iteration   65 => Loss: 18.27758663727619037331\n",
      "Iteration   66 => Loss: 18.03797209528796940958\n",
      "Iteration   67 => Loss: 17.80349769590739938963\n",
      "Iteration   68 => Loss: 17.57404449892519693321\n",
      "Iteration   69 => Loss: 17.34949710397455646671\n",
      "Iteration   70 => Loss: 17.12974347878415670721\n",
      "Iteration   71 => Loss: 16.91467480011580803989\n",
      "Iteration   72 => Loss: 16.70418530627478403972\n",
      "Iteration   73 => Loss: 16.49817216018341881067\n",
      "Iteration   74 => Loss: 16.29653532210032906846\n",
      "Iteration   75 => Loss: 16.09917743115274646470\n",
      "Iteration   76 => Loss: 15.90600369492456600540\n",
      "Iteration   77 => Loss: 15.71692178641254500349\n",
      "Iteration   78 => Loss: 15.53184174772565917522\n",
      "Iteration   79 => Loss: 15.35067589995937886727\n",
      "Iteration   80 => Loss: 15.17333875872841097987\n",
      "Iteration   81 => Loss: 14.99974695488835507717\n",
      "Iteration   82 => Loss: 14.82981916001932987115\n",
      "Iteration   83 => Loss: 14.66347601628251773320\n",
      "Iteration   84 => Loss: 14.50064007029705948071\n",
      "Iteration   85 => Loss: 14.34123571071472014182\n",
      "Iteration   86 => Loss: 14.18518910920014874932\n",
      "Iteration   87 => Loss: 14.03242816454965691264\n",
      "Iteration   88 => Loss: 13.88288244970591200911\n",
      "Iteration   89 => Loss: 13.73648316144713810161\n",
      "Iteration   90 => Loss: 13.59316307254914946157\n",
      "Iteration   91 => Loss: 13.45285648623623941944\n",
      "Iteration   92 => Loss: 13.31549919275329862955\n",
      "Iteration   93 => Loss: 13.18102842790592710287\n",
      "Iteration   94 => Loss: 13.04938283342888460936\n",
      "Iteration   95 => Loss: 12.92050241905512031337\n",
      "Iteration   96 => Loss: 12.79432852616885796238\n",
      "Iteration   97 => Loss: 12.67080379293587633072\n",
      "Iteration   98 => Loss: 12.54987212081350556048\n",
      "Iteration   99 => Loss: 12.43147864235101351937\n",
      "Iteration  100 => Loss: 12.31556969019838199131\n",
      "Iteration  101 => Loss: 12.20209276724858860064\n",
      "Iteration  102 => Loss: 12.09099651784454287906\n",
      "Iteration  103 => Loss: 11.98223069998744882980\n",
      "Iteration  104 => Loss: 11.87574615848847869870\n",
      "Iteration  105 => Loss: 11.77149479901047968156\n",
      "Iteration  106 => Loss: 11.66942956295059374838\n",
      "Iteration  107 => Loss: 11.56950440311834071849\n",
      "Iteration  108 => Loss: 11.47167426016752855844\n",
      "Iteration  109 => Loss: 11.37589503974357540983\n",
      "Iteration  110 => Loss: 11.28212359031037514967\n",
      "Iteration  111 => Loss: 11.19031768162394335775\n",
      "Iteration  112 => Loss: 11.10043598382224594445\n",
      "Iteration  113 => Loss: 11.01243804710300011607\n",
      "Iteration  114 => Loss: 10.92628428196310075293\n",
      "Iteration  115 => Loss: 10.84193593997518156868\n",
      "Iteration  116 => Loss: 10.75935509507889342729\n",
      "Iteration  117 => Loss: 10.67850462536519451362\n",
      "Iteration  118 => Loss: 10.59934819533446415107\n",
      "Iteration  119 => Loss: 10.52185023860942791885\n",
      "Iteration  120 => Loss: 10.44597594108608973329\n",
      "Iteration  121 => Loss: 10.37169122450600333707\n",
      "Iteration  122 => Loss: 10.29896273043494403510\n",
      "Iteration  123 => Loss: 10.22775780463370587370\n",
      "Iteration  124 => Loss: 10.15804448180734809171\n",
      "Iteration  125 => Loss: 10.08979147072033377697\n",
      "Iteration  126 => Loss: 10.02296813966565558474\n",
      "Iteration  127 => Loss: 9.95754450227640752757\n",
      "Iteration  128 => Loss: 9.89349120366926726433\n",
      "Iteration  129 => Loss: 9.83077950690959667668\n",
      "Iteration  130 => Loss: 9.76938127978855597178\n",
      "Iteration  131 => Loss: 9.70926898190307774428\n",
      "Iteration  132 => Loss: 9.65041565202981566074\n",
      "Iteration  133 => Loss: 9.59279489578493915758\n",
      "Iteration  134 => Loss: 9.53638087356158692387\n",
      "Iteration  135 => Loss: 9.48114828873755932648\n",
      "Iteration  136 => Loss: 9.42707237614581927687\n",
      "Iteration  137 => Loss: 9.37412889080097144756\n",
      "Iteration  138 => Loss: 9.32229409687483645541\n",
      "Iteration  139 => Loss: 9.27154475691484414313\n",
      "Iteration  140 => Loss: 9.22185812129901982814\n",
      "Iteration  141 => Loss: 9.17321191792150969491\n",
      "Iteration  142 => Loss: 9.12558434210311197887\n",
      "Iteration  143 => Loss: 9.07895404672107275701\n",
      "Iteration  144 => Loss: 9.03330013255295760644\n",
      "Iteration  145 => Loss: 8.98860213882930203511\n",
      "Iteration  146 => Loss: 8.94484003399025873193\n",
      "Iteration  147 => Loss: 8.90199420664118790114\n",
      "Iteration  148 => Loss: 8.86004545670268051083\n",
      "Iteration  149 => Loss: 8.81897498675035507176\n",
      "Iteration  150 => Loss: 8.77876439354023396788\n",
      "Iteration  151 => Loss: 8.73939565971519805032\n",
      "Iteration  152 => Loss: 8.70085114568854045558\n",
      "Iteration  153 => Loss: 8.66311358170058731787\n",
      "Iteration  154 => Loss: 8.62616606004442232347\n",
      "Iteration  155 => Loss: 8.58999202745695278338\n",
      "Iteration  156 => Loss: 8.55457527767164194188\n",
      "Iteration  157 => Loss: 8.51989994412928552947\n",
      "Iteration  158 => Loss: 8.48595049284330826822\n",
      "Iteration  159 => Loss: 8.45271171541623189682\n",
      "Iteration  160 => Loss: 8.42016872220394141380\n",
      "Iteration  161 => Loss: 8.38830693562459650536\n",
      "Iteration  162 => Loss: 8.35711208360883972546\n",
      "Iteration  163 => Loss: 8.32657019318846280953\n",
      "Iteration  164 => Loss: 8.29666758422038519427\n",
      "Iteration  165 => Loss: 8.26739086324298533270\n",
      "Iteration  166 => Loss: 8.23872691746209184771\n",
      "Iteration  167 => Loss: 8.21066290886361116463\n",
      "Iteration  168 => Loss: 8.18318626845027807803\n",
      "Iteration  169 => Loss: 8.15628469059972083244\n",
      "Iteration  170 => Loss: 8.12994612754133250121\n",
      "Iteration  171 => Loss: 8.10415878394930899731\n",
      "Iteration  172 => Loss: 8.07891111164949649037\n",
      "Iteration  173 => Loss: 8.05419180443751159260\n",
      "Iteration  174 => Loss: 8.02998979300580906227\n",
      "Iteration  175 => Loss: 8.00629423997742684094\n",
      "Iteration  176 => Loss: 7.98309453504396060453\n",
      "Iteration  177 => Loss: 7.96038029020586801465\n",
      "Iteration  178 => Loss: 7.93814133511256780906\n",
      "Iteration  179 => Loss: 7.91636771250052451165\n",
      "Iteration  180 => Loss: 7.89504967372706722983\n",
      "Iteration  181 => Loss: 7.87417767439796989493\n",
      "Iteration  182 => Loss: 7.85374237008679365601\n",
      "Iteration  183 => Loss: 7.83373461214410315989\n",
      "Iteration  184 => Loss: 7.81414544359463025813\n",
      "Iteration  185 => Loss: 7.79496609512042759604\n",
      "Iteration  186 => Loss: 7.77618798112845954762\n",
      "Iteration  187 => Loss: 7.75780269590048376926\n",
      "Iteration  188 => Loss: 7.73980200982388133468\n",
      "Iteration  189 => Loss: 7.72217786570140152236\n",
      "Iteration  190 => Loss: 7.70492237513834243856\n",
      "Iteration  191 => Loss: 7.68802781500552523397\n",
      "Iteration  192 => Loss: 7.67148662397638059218\n",
      "Iteration  193 => Loss: 7.65529139913667755479\n",
      "Iteration  194 => Loss: 7.63943489266529685011\n",
      "Iteration  195 => Loss: 7.62391000858464717993\n",
      "Iteration  196 => Loss: 7.60870979957913196046\n",
      "Iteration  197 => Loss: 7.59382746388037421781\n",
      "Iteration  198 => Loss: 7.57925634221769772836\n",
      "Iteration  199 => Loss: 7.56498991483251526091\n",
      "Iteration  200 => Loss: 7.55102179855533695019\n",
      "Iteration  201 => Loss: 7.53734574394399015063\n",
      "Iteration  202 => Loss: 7.52395563248188548044\n",
      "Iteration  203 => Loss: 7.51084547383491862860\n",
      "Iteration  204 => Loss: 7.49800940316591546519\n",
      "Iteration  205 => Loss: 7.48544167850535036024\n",
      "Iteration  206 => Loss: 7.47313667817714399888\n",
      "Iteration  207 => Loss: 7.46108889827843047016\n",
      "Iteration  208 => Loss: 7.44929295021210968741\n",
      "Iteration  209 => Loss: 7.43774355827111310902\n",
      "Iteration  210 => Loss: 7.42643555727330806349\n",
      "Iteration  211 => Loss: 7.41536389024594111419\n",
      "Iteration  212 => Loss: 7.40452360615859017656\n",
      "Iteration  213 => Loss: 7.39390985770367681340\n",
      "Iteration  214 => Loss: 7.38351789912336542443\n",
      "Iteration  215 => Loss: 7.37334308408215211017\n",
      "Iteration  216 => Loss: 7.36338086358390331299\n",
      "Iteration  217 => Loss: 7.35362678393257773735\n",
      "Iteration  218 => Loss: 7.34407648473567498115\n",
      "Iteration  219 => Loss: 7.33472569694956799680\n",
      "Iteration  220 => Loss: 7.32557024096567577232\n",
      "Iteration  221 => Loss: 7.31660602473683763236\n",
      "Iteration  222 => Loss: 7.30782904194288462918\n",
      "Iteration  223 => Loss: 7.29923537019468327003\n",
      "Iteration  224 => Loss: 7.29082116927576961984\n",
      "Iteration  225 => Loss: 7.28258267942085435465\n",
      "Iteration  226 => Loss: 7.27451621963035677254\n",
      "Iteration  227 => Loss: 7.26661818602031495118\n",
      "Iteration  228 => Loss: 7.25888505020677676782\n",
      "Iteration  229 => Loss: 7.25131335772413709861\n",
      "Iteration  230 => Loss: 7.24389972647654278859\n",
      "Iteration  231 => Loss: 7.23664084522174544389\n",
      "Iteration  232 => Loss: 7.22953347208676078139\n",
      "Iteration  233 => Loss: 7.22257443311459024216\n",
      "Iteration  234 => Loss: 7.21576062084140179564\n",
      "Iteration  235 => Loss: 7.20908899290350735356\n",
      "Iteration  236 => Loss: 7.20255657067354704282\n",
      "Iteration  237 => Loss: 7.19616043792521420386\n",
      "Iteration  238 => Loss: 7.18989773952598998363\n",
      "Iteration  239 => Loss: 7.18376568015723737659\n",
      "Iteration  240 => Loss: 7.17776152306111647761\n",
      "Iteration  241 => Loss: 7.17188258881377738163\n",
      "Iteration  242 => Loss: 7.16612625412420367610\n",
      "Iteration  243 => Loss: 7.16048995065825177875\n",
      "Iteration  244 => Loss: 7.15497116388733545023\n",
      "Iteration  245 => Loss: 7.14956743196123944983\n",
      "Iteration  246 => Loss: 7.14427634460446636666\n",
      "Iteration  247 => Loss: 7.13909554203582263909\n",
      "Iteration  248 => Loss: 7.13402271391055187166\n",
      "Iteration  249 => Loss: 7.12905559828466106609\n",
      "Iteration  250 => Loss: 7.12419198060096281466\n",
      "Iteration  251 => Loss: 7.11942969269630943074\n",
      "Iteration  252 => Loss: 7.11476661182968239672\n",
      "Iteration  253 => Loss: 7.11020065973059622877\n",
      "Iteration  254 => Loss: 7.10572980166747125708\n",
      "Iteration  255 => Loss: 7.10135204553550458684\n",
      "Iteration  256 => Loss: 7.09706544096365199437\n",
      "Iteration  257 => Loss: 7.09286807844035127602\n",
      "Iteration  258 => Loss: 7.08875808845747101827\n",
      "Iteration  259 => Loss: 7.08473364067230360064\n",
      "Iteration  260 => Loss: 7.08079294308706508332\n",
      "Iteration  261 => Loss: 7.07693424124556980104\n",
      "Iteration  262 => Loss: 7.07315581744674126696\n",
      "Iteration  263 => Loss: 7.06945598997460500357\n",
      "Iteration  264 => Loss: 7.06583311234447197791\n",
      "Iteration  265 => Loss: 7.06228557256476019433\n",
      "Iteration  266 => Loss: 7.05881179241447043182\n",
      "Iteration  267 => Loss: 7.05541022673563578138\n",
      "Iteration  268 => Loss: 7.05207936274073610150\n",
      "Iteration  269 => Loss: 7.04881771933446099609\n",
      "Iteration  270 => Loss: 7.04562384644984174287\n",
      "Iteration  271 => Loss: 7.04249632439822015328\n",
      "Iteration  272 => Loss: 7.03943376323276748252\n",
      "Iteration  273 => Loss: 7.03643480212549210506\n",
      "Iteration  274 => Loss: 7.03349810875714798186\n",
      "Iteration  275 => Loss: 7.03062237872002171457\n",
      "Iteration  276 => Loss: 7.02780633493317186122\n",
      "Iteration  277 => Loss: 7.02504872706998551024\n",
      "Iteration  278 => Loss: 7.02234833099763999797\n",
      "Iteration  279 => Loss: 7.01970394822837295834\n",
      "Iteration  280 => Loss: 7.01711440538223918395\n",
      "Iteration  281 => Loss: 7.01457855366109939155\n",
      "Iteration  282 => Loss: 7.01209526833364904519\n",
      "Iteration  283 => Loss: 7.00966344823121012553\n",
      "Iteration  284 => Loss: 7.00728201525413574302\n",
      "Iteration  285 => Loss: 7.00494991388849364000\n",
      "Iteration  286 => Loss: 7.00266611073292288836\n",
      "Iteration  287 => Loss: 7.00042959403537778940\n",
      "Iteration  288 => Loss: 6.99823937323957956380\n",
      "Iteration  289 => Loss: 6.99609447854099553155\n",
      "Iteration  290 => Loss: 6.99399396045205445915\n",
      "Iteration  291 => Loss: 6.99193688937656521176\n",
      "Iteration  292 => Loss: 6.98992235519297366864\n",
      "Iteration  293 => Loss: 6.98794946684637618972\n",
      "Iteration  294 => Loss: 6.98601735194907913495\n",
      "Iteration  295 => Loss: 6.98412515638956854502\n",
      "Iteration  296 => Loss: 6.98227204394955425215\n",
      "Iteration  297 => Loss: 6.98045719592919144958\n",
      "Iteration  298 => Loss: 6.97867981078001786699\n",
      "Iteration  299 => Loss: 6.97693910374570336330\n",
      "\n",
      "Weights: [[-5.69880698e-04]\n",
      " [ 1.12888235e+00]\n",
      " [ 1.95815830e-01]\n",
      " [ 2.99481375e+00]]\n",
      "\n",
      "A few predictions:\n",
      "X[0] -> 46.7194 (label: 44)\n",
      "X[1] -> 22.9675 (label: 23)\n",
      "X[2] -> 28.7045 (label: 28)\n",
      "X[3] -> 57.8124 (label: 60)\n",
      "X[4] -> 43.3330 (label: 42)\n"
     ]
    }
   ],
   "source": [
    "x1, x2, x3, y = np.loadtxt(\"pizza_3_vars.txt\", skiprows=1, unpack=True)\n",
    "X = np.column_stack((np.ones(x1.size), x1, x2, x3))\n",
    "Y = y.reshape(-1, 1)\n",
    "regressor = LinearRegression(learning_rate=0.001, n_iters=300)\n",
    "weights=regressor.train(X, Y)\n",
    "\n",
    "print(\"\\nWeights: %s\" % weights)\n",
    "print(\"\\nA few predictions:\")\n",
    "for i in range(5):\n",
    "    print(\"X[%d] -> %.4f (label: %d)\" % (i, regressor.predict(X[i]), Y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "gca() got an unexpected keyword argument 'projection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15188\\10706197.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Plot the axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"axes.facecolor\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"white\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"figure.facecolor\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"white\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"3d\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Temperature\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Reservations\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: gca() got an unexpected keyword argument 'projection'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot a plane that roughly approximates a dataset with two input variables.\n",
    "# Plot the axes\n",
    "sns.set(rc={\"axes.facecolor\": \"white\", \"figure.facecolor\": \"white\"})\n",
    "ax = plt.figure().gca(projection=\"3d\")\n",
    "ax.set_xlabel(\"Temperature\", labelpad=15, fontsize=30)\n",
    "ax.set_ylabel(\"Reservations\", labelpad=15, fontsize=30)\n",
    "ax.set_zlabel(\"Pizzas\", labelpad=5, fontsize=30)\n",
    "\n",
    "# Plot the data points\n",
    "ax.scatter(x1, x2, y, color='b')\n",
    "\n",
    "w=weights.T\n",
    "print(w)\n",
    "\n",
    "# Plot the plane\n",
    "MARGIN = 10\n",
    "edges_x = [np.min(x1) - MARGIN, np.max(x1) + MARGIN]\n",
    "edges_y = [np.min(x2) - MARGIN, np.max(x2) + MARGIN]\n",
    "print(edges_x)\n",
    "xs, ys = np.meshgrid(edges_x, edges_y)\n",
    "print(xs)\n",
    "print(np.ravel(xs)) # 將 xs 拉成一維數列 \n",
    "print([[x, y] for x, y in zip(np.ravel(xs), np.ravel(ys))])\n",
    "zs = np.array([w[0,0] + x * w[0,1] + y * w[0,2] for x, y in\n",
    "              zip(np.ravel(xs), np.ravel(ys))])\n",
    "ax.plot_surface(xs, ys, zs.reshape((2, 2)), alpha=0.2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
