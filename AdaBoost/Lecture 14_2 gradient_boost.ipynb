{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-2-classification-d3ed8f56541e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class CustomGradientBoostingClassifier:\n",
    "    \n",
    "    def __init__(self, learning_rate, n_estimators, max_depth=1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.y_hat = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        F0 = np.log(y.mean()/(1-y.mean()))  # log-odds values\n",
    "        self.y_hat = F0\n",
    "        self.F0 = np.full(len(y), F0)  # converting to array with the input length\n",
    "        Fm = self.F0.copy()\n",
    "        \n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            p = np.exp(Fm) / (1 + np.exp(Fm))  # converting back to probabilities\n",
    "            r = y - p  # residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=0)\n",
    "            tree.fit(X, r)\n",
    "            ids = tree.apply(X)  # getting the terminal node IDs\n",
    "\n",
    "            # looping through the terminal nodes \n",
    "            for j in np.unique(ids):\n",
    "              fltr = ids == j\n",
    "\n",
    "              # getting gamma using the formula (Σresiduals/Σp(1-p))\n",
    "              num = r[fltr].sum()\n",
    "              den = (p[fltr]*(1-p[fltr])).sum()\n",
    "              gamma = num / den\n",
    "\n",
    "              # updating the prediction\n",
    "              Fm[fltr] += self.learning_rate * gamma\n",
    "\n",
    "              # replacing the prediction value in the tree\n",
    "              tree.tree_.value[j, 0, 0] = gamma\n",
    "\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict_proba(self, X):        \n",
    "        F0 = self.y_hat  \n",
    "        Fm = np.full(len(X), F0)\n",
    "        #print(Fm)\n",
    "        for i in range(self.n_estimators):\n",
    "            Fm += self.learning_rate * self.trees[i].predict(X)\n",
    "            \n",
    "        return np.exp(Fm) / (1 + np.exp(Fm))  # converting back into probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (455,)\n",
      "(114, 30) (114,)\n",
      "Custom GBM Log-Loss:0.033312747890304\n",
      "[9.98995535e-01 9.83374221e-01 4.54413984e-02 9.19879858e-01\n",
      " 2.10518715e-03 9.95901821e-01 9.99602707e-01 9.94978062e-01\n",
      " 4.58403683e-03 9.96496671e-01 4.76836524e-03 9.99690879e-01\n",
      " 2.69796044e-03 9.98530703e-01 9.90947265e-01 7.53311861e-02\n",
      " 9.43232422e-01 9.95792904e-01 9.95897917e-01 9.49683990e-01\n",
      " 9.84194435e-01 2.31542984e-02 9.88058999e-01 9.97962012e-01\n",
      " 9.96468424e-01 9.99652401e-01 9.88314899e-01 9.55074325e-01\n",
      " 9.00035547e-01 6.09232507e-03 9.98254096e-01 8.38435963e-01\n",
      " 9.86830159e-01 1.82240901e-03 9.92804245e-01 9.99662945e-01\n",
      " 9.87254367e-01 4.13923246e-02 9.98925285e-01 9.94249837e-01\n",
      " 9.99178148e-01 5.15200902e-04 9.98729141e-01 9.46276090e-01\n",
      " 8.05199276e-01 9.70499731e-01 9.98797073e-01 9.97791107e-01\n",
      " 9.99497260e-01 9.99487618e-01 1.12854388e-03 2.69796044e-03\n",
      " 9.86832476e-01 9.98854146e-01 9.36029745e-01 9.99120184e-01\n",
      " 9.96047409e-01 1.18065579e-01 9.99487618e-01 3.83733488e-02\n",
      " 9.99521128e-01 9.55805601e-01 9.99216913e-01 9.91509713e-01\n",
      " 9.99077320e-01 5.15669030e-02 9.92007087e-01 9.99031352e-01\n",
      " 9.94528370e-01 6.86074259e-03 9.69835746e-01 5.80403698e-03\n",
      " 9.98774020e-01 9.97537690e-01 9.85838831e-01 4.68189251e-02\n",
      " 9.83851709e-01 5.43361338e-03 7.34259166e-01 9.99487618e-01\n",
      " 6.09232507e-03 9.98953423e-01 9.99156891e-01 9.87851133e-01\n",
      " 2.39231850e-03 3.82276515e-02 3.17588407e-03 4.76046458e-02\n",
      " 9.97992554e-01 9.94767174e-01 9.96635608e-01 6.57765527e-04\n",
      " 9.98774020e-01 9.97253718e-01 2.44909812e-03 9.98774020e-01\n",
      " 9.21902700e-01 9.89321774e-01 9.99171923e-01 9.70551364e-01\n",
      " 9.97149371e-01 9.55653289e-01 9.98053347e-01 9.98700319e-01\n",
      " 9.08685561e-01 9.96239113e-01 9.97131007e-01 9.93646618e-01\n",
      " 9.92229227e-01 1.64453902e-03 3.52376313e-04 9.96750435e-01\n",
      " 9.99131371e-01 9.99587398e-01 2.83816764e-03 3.85072443e-03\n",
      " 9.93775788e-01 1.20746116e-03 9.94762295e-01 6.99752911e-01\n",
      " 9.90784798e-01 9.98434966e-01 9.95975576e-01 9.90878696e-01\n",
      " 1.49869276e-02 9.98993244e-01 3.41195349e-03 3.52807504e-03\n",
      " 9.97846678e-01 2.55938307e-03 9.66686178e-01 9.92410795e-01\n",
      " 5.69596348e-03 9.38973489e-01 4.65213429e-02 9.99649767e-01\n",
      " 9.86809170e-01 3.64996248e-03 9.98860668e-01 9.95326796e-01\n",
      " 9.98460655e-01 9.96970681e-01 9.98457793e-01 9.99315963e-01\n",
      " 1.09459149e-01 9.89925016e-01 8.33977254e-03 9.58790977e-01\n",
      " 1.43484641e-01 1.48326252e-02 9.74316335e-01 9.98807050e-01\n",
      " 9.97042370e-01 7.03208153e-01 1.48792840e-01 9.59628010e-01\n",
      " 9.98340918e-01 9.99126798e-01 9.99605695e-01 9.99605695e-01\n",
      " 7.82829369e-04 4.79677887e-02 9.95729096e-01 3.17588407e-03\n",
      " 9.98793992e-01 4.27904630e-02 1.56630552e-02 8.26640535e-03\n",
      " 9.86555149e-01 9.98953423e-01 1.29542168e-03 7.27196240e-01\n",
      " 3.98171933e-03 9.98434966e-01 9.99458574e-01 9.94728161e-01\n",
      " 9.93100062e-01 9.99451842e-01 9.99117483e-01 4.06891219e-03\n",
      " 7.27419997e-01 8.42549445e-03 1.49565228e-03 9.86405698e-01\n",
      " 9.97692479e-01 9.98640285e-01 9.96739771e-01 9.72761643e-01\n",
      " 9.96604115e-01 2.09271108e-02 2.61700282e-03 9.84194435e-01\n",
      " 9.36637514e-01 9.96227224e-01 1.08812678e-01 7.37011788e-02\n",
      " 1.06702525e-03 9.06428523e-01 7.52680864e-01 9.98995535e-01\n",
      " 5.69596348e-03 2.44801103e-01 9.96315234e-01 9.89925016e-01\n",
      " 9.99126798e-01 9.99521128e-01 3.78444593e-03 9.98838703e-01\n",
      " 1.18177239e-03 9.97675802e-01 9.98340918e-01 3.25770217e-03\n",
      " 9.52111112e-01 8.22499590e-01 9.92229227e-01 9.99178148e-01\n",
      " 2.59962506e-02 9.77269242e-01 1.71678239e-02 3.04519375e-03\n",
      " 9.96635608e-01 9.99462080e-01 9.20250359e-01 6.78436330e-03\n",
      " 9.98375796e-01 1.88440802e-02 1.65073198e-01 9.98860668e-01\n",
      " 9.94809128e-01 9.99662945e-01 7.26050928e-01 1.54434670e-02\n",
      " 9.99275461e-01 9.98953423e-01 9.99607029e-01 3.58440611e-03\n",
      " 2.39231850e-03 6.35209591e-03 3.49900503e-04 4.23394674e-04\n",
      " 9.94502384e-01 9.98857061e-01 9.98340918e-01 8.65811925e-01\n",
      " 9.81592971e-01 9.99727080e-01 9.97188180e-01 9.95071407e-01\n",
      " 9.96309461e-01 9.98464385e-01 9.99554183e-01 7.63710550e-02\n",
      " 9.93790538e-01 9.96109942e-01 9.98807050e-01 3.50336484e-03\n",
      " 9.97159037e-01 9.80179417e-01 3.52376313e-04 1.82240901e-03\n",
      " 9.98945497e-01 9.99727080e-01 9.94348122e-01 9.83200450e-01\n",
      " 8.76174656e-01 9.86626408e-02 9.98741502e-01 8.45272781e-01\n",
      " 7.58203617e-01 9.81847526e-01 9.43431578e-01 1.12854388e-03\n",
      " 9.98378606e-01 9.96315234e-01 3.52376313e-04 8.54531550e-01\n",
      " 5.29621677e-03 9.99385840e-01 2.69796044e-03 2.25224554e-03\n",
      " 9.96088000e-01 9.98925285e-01 1.47792283e-01 9.75268632e-01\n",
      " 8.33103679e-03 9.98688584e-01 2.54558206e-02 9.98871751e-01\n",
      " 9.97027543e-01 9.98144990e-01 3.40130889e-01 1.14164554e-03\n",
      " 1.32775787e-02 9.97361619e-01 2.71356782e-02 9.85866100e-01\n",
      " 9.94305520e-01 9.97554625e-01 9.92511379e-01 2.40160709e-02\n",
      " 2.79162911e-01 1.78969328e-01 9.99602707e-01 1.04388048e-01\n",
      " 9.98712673e-01 9.98865293e-01 8.43321913e-04 1.47079046e-01\n",
      " 9.42667184e-02 9.97843139e-01 9.98109460e-01 9.99497260e-01\n",
      " 9.98640285e-01 2.25224554e-03 1.73279746e-02 1.12854388e-03\n",
      " 9.94851059e-01 9.99664050e-01 7.50379404e-01 9.98572575e-01\n",
      " 4.95139130e-03 9.98478047e-01 1.46240993e-03 5.70916600e-03\n",
      " 1.94470172e-03 9.97263069e-01 5.78147370e-02 9.98836305e-01\n",
      " 9.84883304e-03 9.99609534e-01 1.12854388e-03 3.81336435e-03\n",
      " 9.86777417e-01 9.86224162e-01 9.99521128e-01 2.88727110e-02\n",
      " 9.98871751e-01 9.98701339e-01 9.71341244e-01 9.91492576e-01\n",
      " 6.82389523e-02 9.81322870e-01 3.77037913e-04 3.26120919e-04\n",
      " 1.21076245e-03 9.96750435e-01 2.05700668e-02 9.98457793e-01\n",
      " 1.60625446e-02 3.52376313e-04 9.99219414e-01 9.96234552e-01\n",
      " 6.07554689e-04 9.98924796e-01 1.13060911e-01 9.94704373e-01\n",
      " 9.95930142e-01 3.52376313e-04 5.30120514e-03 9.99456793e-01\n",
      " 9.92507957e-01 1.12854388e-03 9.98557428e-01 1.04297946e-01\n",
      " 8.85447533e-01 9.76839297e-01 9.98774020e-01 9.81162052e-01\n",
      " 7.75375404e-02 9.77427561e-01 9.98052682e-01 9.93998514e-01\n",
      " 9.96788921e-01 1.12854388e-03 9.11916183e-01 2.71094627e-03\n",
      " 9.75773672e-01 9.83922433e-01 9.54998694e-01 9.90142712e-01\n",
      " 1.36510897e-01 9.98823751e-01 9.98232943e-01 1.04451704e-03\n",
      " 1.90805259e-03 2.83816764e-03 4.40456608e-03 9.94797809e-01\n",
      " 3.52807504e-03 9.98764461e-01 9.95084795e-01 9.98561233e-01\n",
      " 9.99621571e-01 9.99111539e-01 9.44777807e-01 1.49869276e-02\n",
      " 9.94728161e-01 1.12854388e-03 2.76134137e-02 9.87922480e-01\n",
      " 9.97968687e-01 9.99031352e-01 2.39231850e-03 9.99456793e-01\n",
      " 2.88630738e-03 9.81800602e-01 3.49900503e-04 5.89356929e-02\n",
      " 4.95139130e-03 9.96811418e-01 1.12854388e-03 9.97950963e-01\n",
      " 9.99688059e-01 3.13879014e-03 9.34848611e-01 8.87287633e-01\n",
      " 1.43491637e-02 9.98101347e-01 7.42016357e-01 9.94051639e-01\n",
      " 7.22280391e-02 9.40864032e-01 6.11573585e-02 9.99521128e-01\n",
      " 9.98724962e-01 9.51944263e-01 9.96811418e-01 1.58353277e-01\n",
      " 9.99327221e-01 9.85933474e-01 3.41195349e-03 3.78069623e-04\n",
      " 2.09591968e-01 9.97935707e-01 9.76304329e-01 4.95139130e-03\n",
      " 1.54125467e-01 2.09981773e-01 9.28783889e-01 9.91452094e-01\n",
      " 1.51186834e-03 1.64555422e-01 6.07492712e-02 9.13232595e-03\n",
      " 9.88528708e-01 9.96153108e-01 9.98464400e-01 9.74352864e-01\n",
      " 9.96315234e-01 9.97298412e-01 4.45806708e-02 4.17959383e-01\n",
      " 1.18177239e-03 7.82829369e-04 9.96750435e-01]\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# y[y == 0] = -1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "custom_gbm = CustomGradientBoostingClassifier(\n",
    "    n_estimators=200, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=1\n",
    ")\n",
    "custom_gbm.fit(X_train, y_train)\n",
    "prob_train = custom_gbm.predict_proba(X_train)\n",
    "custom_gbm_log_loss = log_loss(y_train, prob_train)\n",
    "print(f\"Custom GBM Log-Loss:{custom_gbm_log_loss:.15f}\")\n",
    "print(prob_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1]\n",
      "Accuracy: 0.9978021978021978\n"
     ]
    }
   ],
   "source": [
    "y_pred=[1 if x > 0.5 else 0 for x in prob_train]\n",
    "print(y_pred)\n",
    "\n",
    "acc = accuracy(y_train, y_pred)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "prob_test = custom_gbm.predict_proba(X_test)\n",
    "\n",
    "y_pred=[1 if x > 0.5 else 0 for x in prob_test]\n",
    "print(y_pred)\n",
    "\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
